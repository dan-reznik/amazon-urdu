---
title: "Urdu Language Sentiment Classifier"
output: html_document
---

```{r}
library(tidyverse)
library(tm)
library(xgboost)
library(h2o)
```

# Analyze input file

Source file

```{r}
fname <- "data/Roman Urdu DataSet.csv"
```

Encoding is UTF-8

```{r}
guess_encoding(fname)
```

Look at the first few lines of file:

- comma separated, header is missing

```{r}
read_lines(fname,n_max = 3)
```

Reads 3-col csv as characters

```{r}
df_urdu <- read_csv("data/Roman Urdu DataSet.csv",col_names = c("phrase","sentiment","bogus"),
                    col_types = "ccc", # all are chars
                    #n_max=3
                    )
df_urdu %>% glimpse
```

Third column can truly be ignored

```{r}
df_urdu %>% count(bogus,sort=T)
```

Categories on sentiment column:

```{r}
df_urdu %>%
  ggplot(aes(sentiment,fill=sentiment)) +
  geom_bar()
```

One sacred line with "Neative", what is it?

```{r}
df_urdu %>% filter(sentiment == "Neative")
```
A few lines have NA phrases, which need to be removed.

```{r}
df_urdu %>%
  filter(is.na(phrase))
```
# Study characters in the set

```{r}
df_urdu$phrase[1] %>% str_split("")
```

Frequency count of all chars used

```{r}
count_chars <- function(df,col) {
  df %>%  # head(10) %>%
  mutate(chars = {{col}} %>% str_split("")) %>%
  select(chars) %>%
  unnest(chars) %>%
  count(chars,sort=T) %>%
  mutate(prop=sprintf("%.3f",n/sum(n)))  
}
```

```{r}
df_char_freq <- df_urdu %>% count_chars(phrase)
df_char_freq%>%glimpse
```

Which are non-alpha. Note 0x001F602 doesn't exist.

```{r}
df_char_freq %>% filter(!str_detect(chars,"[:alpha:]"))
```
Preprocessing steps:

- eliminate NA phrases
- change 'Neative' to 'Negative'
- eliminate non-alpha (includes numbers)
- convert all to lower-case
- squish multiple spaces

```{r}
df_urdu_clean <- df_urdu %>%
  filter(!is.na(phrase)) %>%
  mutate(sentiment=if_else(sentiment=="Neative","Negative",sentiment)) %>%
  mutate(phrase_clean=phrase %>%
           str_remove_all("[^ [:alpha:]]") %>%
           str_to_lower() %>%
           str_squish()) %>%
  select(phrase_clean,sentiment)
df_urdu_clean %>% head(10)
```
Confirm chars are ok

```{r}
df_urdu_clean %>% count_chars(phrase_clean)
```
# Estudo de tokens

```{r}
df_urdu_tokens <- df_urdu_clean %>%
  mutate(token = str_split(phrase_clean,fixed(" "))) %>%
  select(token) %>%
  unnest(token) %>%
  count(token,sort=T) %>%
  mutate(id=row_number(),
         prop=n/sum(n),
         propSum=cumsum(prop))
df_urdu_tokens
```

# Document Term Matrix

Create term matrix. Each rows has the count of the top N tokens

```{r}
corpus_urdu <- SimpleCorpus(VectorSource(df_urdu_clean$phrase_clean))
```

Creat a document term matrix

```{r}
fn_tf_idf <- function(x) weightTfIdf(x, normalize = F)

dtm_urdu <- DocumentTermMatrix(corpus_urdu
                               #, control = list(weighting = tfidf_fn)
                               )
dtm_urdu
```

Inspect first five lines and first 10 columns

```{r}
mtx <- inspect(dtm_urdu[1:10,1:100]) %>% as.matrix
mtx %>% dim
```

Note: 99.8% sparsity => 673 columns, AUC ~ 80%
995 => ~200 columns, AUC falls to 75%

```{r}
df_urdu_dtm <- removeSparseTerms(dtm_urdu, 0.998) %>% as.matrix %>% as_tibble
df_urdu_dtm %>% dim
```

# H2O PCA and ML

```{r}
h2o.init()
```

XGBoost only available on mac or linux

```{r}
h2o.xgboost.available()
```

Slow: do any of the columns have NA

```{r,eval=F}
any_na <- function(vals) any(is.na(vals))

df_urdu_mtx %>% as_tibble %>%
  mutate(has_na=pmap_lgl(.,~any_na(list(...)))) %>%
  filter(has_na)
```
Convert to h2o matrix

Bring sentiment column: note, removing neutral and allocate twice the weight to Negative

```{r}
df_urdu_dtm_y <- df_urdu_dtm %>%
  bind_cols(df_urdu_clean %>% select(sentiment) %>%
              mutate_at(vars(sentiment),as.factor)) %>%
  # double the weight on negatives
  mutate(weight=if_else(sentiment=="Negative",2,1)) %>%
  select(sentiment,weight,everything()) %>%
  filter(sentiment!="Neutral") %>%
  mutate(sentiment=sentiment=="Negative")
```

Cast to h2o data frame

```{r}
df_urdu_h2o <- df_urdu_dtm_y %>%
  as.h2o()
df_urdu_h2o
```

Ideias para amanha:

- should make separate classifier for neutral?
- should we combine neutral + positive? (i.e., only cares about detecting negatives)
- We could do a correlation table (perceptron-like)
- Add xgboost

```{r,eval=F}
ml_urdu <- h2o::h2o.automl(y="sentiment",
                           weights_column = "weight",
                           training_frame=df_urdu_h2o,
                           max_runtime_secs = 3600,
                           seed=0,
                           stopping_metric = "misclassification")
```

Which models on leaderboard?

```{r}
ml_urdu@leaderboard$model_id
```

```{r}
ml_urdu@leaderboard$model_id %>%
  map(~{h2o.saveModel(object=h2o.getModel(.x),
                      path="models",force=T)})
```


has the list of all model ids that were built. You can then pass each of those to h2o.getModel() (which you can then call h2o.saveModel() on).
```


```{r}
h2o.saveModel()
ml_urdu %>% write_rds()
```


```{r}
h2o.get_leaderboard(ml_urdu)
```

```{r}
h2o.varimp_plot(h2o.getModel("GLM_1_AutoML_20200628_181701"))
```


```{r}
h2o.varimp_plot(ml_urdu@leader)
```


```{r}
h2o::h2o.shutdown()
```

## PCA

```{r}
urdu_pca <- h2o::h2o.prcomp(mtx_h2o,k=20)
urdu_pca
```

# Business outcomes

Sentiment analysis in urdu could help one automatically gauge the collective sentiment of tweets, posts in a brand page, and chat channels, and other natural language media, for the purposes of sizing, training, and focusing customer care. Another application might be for one to choose good marketing messages and/or price digital marketing real-estate.

# Data Limitations

The sample is much too small (20k snippets), though the quality is generally good. Another approach might be to translate the words in urdu into english and use english positive/negative sentiment. Yet another approach might be to build large correlation matrices of each of the ~32k words and the positive/negative/neutral labels.
